plot(PCDiabetes[,1], PCDiabetes[,3],pch=20,col=my_cols[pam2.diabetes$clustering])
my_cols <- c("deepskyblue2","firebrick2")
plot(PCDiabetes[,1], PCDiabetes[,3],pch=20,col=my_cols[diabetes$class])
## This is the one that looks the worst of the three
my_cols <- c("firebrick2","deepskyblue2")
plot(PCDiabetes[,2], PCDiabetes[,3],pch=20,col=my_cols[pam2.diabetes$clustering])
my_cols <- c("deepskyblue2","firebrick2")
plot(PCDiabetes[,2], PCDiabetes[,3],pch=20,col=my_cols[diabetes$class])
############################# HOW MANY K's FOR PAM? ########################
## Let's see what should be the K-value according to fviz
## WSS
# In this case the graph becomes straight at apporx K=5
fviz_nbclust(diabetes[-9],pam,method="wss",k.max=10)
# In this case like in the previous one we obtain K=2 as the best option
fviz_nbclust(diabetes[-9],pam, method="silhouette",k.max = 10)
# As before it indicates that we shouldn't be using clustering methods
fviz_nbclust(diabetes[-9],pam, method = "gap",k.max = 10)
###########Para plotear según colores!!#######
summary(as.factor(kmeans.diabetes$cluster))
diabetes$class
for (i in range(length(filt)){
for i in range(length(filt){
for i in 1:length(filt){
print(i)
}
1:length(filt)
for (i in (1:length(filt))){
print(i)
}
for i in (1:length(filt)){
print(i)
}
for (i in (1:length(filt))){
print(i)
}
for (i in (1:length(filt))){
if(class[i]=="tested_negative")
print(i)
}
filt=rep(0,length(kmeans.diabetes$cluster))
for (i in (1:length(filt))){
if((class[i]=="tested_negative")&kmeans.diabetes$cluster[i]==2)
print(i)
}
filt=rep(0,length(kmeans.diabetes$cluster))
for (i in (1:length(filt))){
if((class[i]=="tested_negative")&kmeans.diabetes$cluster[i]!=2)
print(i)
}
filt=rep(0,length(kmeans.diabetes$cluster))
for (i in (1:length(filt))){
if((class[i]=="tested_negative")&kmeans.diabetes$cluster[i]!=2)
filt[i]=1
print(i)
}
filt=rep(0,length(kmeans.diabetes$cluster))
for (i in (1:length(filt))){
if((class[i]=="tested_negative")&kmeans.diabetes$cluster[i]!=2)
filt[i]=1
print(i)
else if ((class[i]=="tested_positive")&kmeans.diabetes$cluster[i]!=1)
}
filt=rep(0,length(kmeans.diabetes$cluster))
for (i in (1:length(filt))){
if((class[i]=="tested_negative")&kmeans.diabetes$cluster[i]!=2)
filt[i]=1
print(i)
}
filt
for (i in (1:length(filt))){
if((class[i]=="tested_negative")&kmeans.diabetes$cluster[i]!=2)
filt[i]=1
print(i)
else if (class[i]="tested_positive")
print("pos")
}
for (i in (1:length(filt))){
if((class[i]=="tested_negative")&kmeans.diabetes$cluster[i]!=2)
filt[i]=1
print(i)
else if (class[i]="tested_positive"){
print("pos")
}
}
for (i in (1:length(filt))){
if((class[i]=="tested_negative")&kmeans.diabetes$cluster[i]!=2){
filt[i]=1
print(i)
}
else if (class[i]="tested_positive"){
print("pos")
}
}
for (i in (1:length(filt))){
if((class[i]=="tested_negative")&kmeans.diabetes$cluster[i]!=2){
filt[i]=1
print(i)
}
else if (class[i]="tested_positive"){
print("pos")
}
}
for (i in (1:length(filt))){
if((class[i]=="tested_negative")&kmeans.diabetes$cluster[i]!=2){
filt[i]=1
print(i)}
else if (class[i]="tested_positive"){
print("pos")}
}
for (i in (1:length(filt))){
if((class[i]=="tested_negative")&kmeans.diabetes$cluster[i]!=2){
filt[i]=1
print(i)}
}
for (i in (1:length(filt))){
if((class[i]=="tested_negative")&kmeans.diabetes$cluster[i]!=2){
filt[i]=1
print(i)}
else if (class[i]="tested_positive"){
print("pos")}
}
for (i in (1:length(filt))){
if((class[i]=="tested_negative")&kmeans.diabetes$cluster[i]!=2){
filt[i]=1
print(i)}
}
class
class=="tested_negative"
kmeans.diabetes$cluste
kmeans.diabetes$cluster
kmeans.diabetes$cluster=2
kmeans.diabetes <- kmeans(diabetes[-9], centers = 2, iter.max = 10000, nstart = 50)
summary(kmeans.diabetes)
kmeans.diabetes$cluster
diabetes$class
kmeans.diabetes$cluster==2
class=="tested_negative"
(class=="tested_negative")==(kmeans.diabetes$cluster==2)
alf=(class=="tested_negative")==(kmeans.diabetes$cluster==2)
summary(alf)
###########Para plotear según colores!!#######
summary(as.factor(kmeans.diabetes$cluster))
sumary(class)
summary(class)
###########Para plotear según colores!!#######
summary(as.factor(kmeans.diabetes$cluster))
chain1=(class=="tested_negative")==(kmeans.diabetes$cluster==2)
chain2=(class=="tested_positive")==(kmeans.diabetes$cluster==1)
chain3=chain1 xor chain2
chain1=(class=="tested_negative")==(kmeans.diabetes$cluster==2)
chain2=(class=="tested_positive")==(kmeans.diabetes$cluster==1)
chain3=chain1 | chain2
chain3
chain1=(class=="tested_negative")!=(kmeans.diabetes$cluster==2)
chain2=(class=="tested_positive")!=(kmeans.diabetes$cluster==1)
chain3=chain1 | chain2
chain3
summary(chain3)
summary(chain3)
chain1=(class=="tested_negative")!=(kmeans.diabetes$cluster==2)
chain2=(class=="tested_positive")!=(kmeans.diabetes$cluster==1)
chain3=chain1 | chain2
summary(chain3)
class(chain3)
plot(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=19,col=my_cols[class[chain3]])
summary(chain3)
plot(xlab="PC1 Vs PC2",PCDiabetes[,1], PCDiabetes[,2], pch=19,col=my_cols[class])
plot(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=19,col=my_cols[class[chain3]])
#2-tested_negative
#1-tested_positive
summary(class)
summary(kmeans.diabetes$cluster)
summary(as.factor(kmeans.diabetes$cluster))
#2-tested_negative
#1-tested_positive
summary(class)
summary(as.factor(kmeans.diabetes$cluster))
summary(as.factor(kmeans.diabetes$cluster))
chain1=(class=="tested_negative")!=(kmeans.diabetes$cluster==1)
chain2=(class=="tested_positive")!=(kmeans.diabetes$cluster==2)
chain3=chain1 | chain2
summary(chain3)
plot(xlab="PC1 Vs PC2",PCDiabetes[,1], PCDiabetes[,2], pch=19,col=my_cols[class])
plot(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=19,col=my_cols[class[chain3]])
points(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=19)
plot(xlab="PC1 Vs PC2",PCDiabetes[,1], PCDiabetes[,2], pch=19,col=my_cols[class])
points(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=4)
points(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=8)
plot(xlab="PC1 Vs PC2",PCDiabetes[,1], PCDiabetes[,2], pch=19,col=my_cols[class])
points(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=8)
plot(xlab="PC1 Vs PC2",PCDiabetes[,1], PCDiabetes[,2], pch=19,col=my_cols[class])
points(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=1)
points(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=13)
points(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=4)
plot(xlab="PC1 Vs PC2",PCDiabetes[,1], PCDiabetes[,2], pch=19,col=my_cols[class])
points(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=4)
plot(xlab="PC1 Vs PC3",PCDiabetes[,1], PCDiabetes[,3], pch=19,col=my_cols[class])
points(xlab="PC1 Vs PC3",PCDiabetes[chain3,1], PCDiabetes[chain3,3], pch=4)
plot(xlab="PC2 Vs PC3",PCDiabetes[,2], PCDiabetes[,3], pch=19,col=my_cols[class])
points(xlab="PC2 Vs PC3",PCDiabetes[chain3,2], PCDiabetes[chain3,3], pch=4)
points(xlab="PC2 Vs PC3",PCDiabetes[chain3,2], PCDiabetes[chain3,3], pch=19)
plot(xlab="PC2 Vs PC3",PCDiabetes[,2], PCDiabetes[,3], pch=19,col=my_cols[class])
points(xlab="PC2 Vs PC3",PCDiabetes[chain3,2], PCDiabetes[chain3,3], pch=19)
points(xlab="PC2 Vs PC3",PCDiabetes[chain3,2], PCDiabetes[chain3,3], pch=20)
plot(xlab="PC2 Vs PC3",PCDiabetes[,2], PCDiabetes[,3], pch=19,col=my_cols[class])
points(xlab="PC2 Vs PC3",PCDiabetes[chain3,2], PCDiabetes[chain3,3], pch=20)
par(mfrow=c(1,3))
plot(xlab="PC1 Vs PC2",PCDiabetes[,1], PCDiabetes[,2], pch=19,col=my_cols[class])
points(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=4)
par(mfrow=c(1,3))
dev.off()
par(mfrow=c(1,3))
plot(xlab="PC1 Vs PC2",PCDiabetes[,1], PCDiabetes[,2], pch=19,col=my_cols[class])
points(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=20)
plot(xlab="PC1 Vs PC3",PCDiabetes[,1], PCDiabetes[,3], pch=19,col=my_cols[class])
points(xlab="PC1 Vs PC3",PCDiabetes[chain3,1], PCDiabetes[chain3,3], pch=4)
dev.off()
par(mfrow=c(1,3))
plot(xlab="PC1 Vs PC3",PCDiabetes[,1], PCDiabetes[,3], pch=19,col=my_cols[class])
points(xlab="PC1 Vs PC3",PCDiabetes[chain3,1], PCDiabetes[chain3,3], pch=20)
plot(xlab="PC2 Vs PC3",PCDiabetes[,2], PCDiabetes[,3], pch=19,col=my_cols[class])
points(xlab="PC2 Vs PC3",PCDiabetes[chain3,2], PCDiabetes[chain3,3], pch=20)
plot(xlab="PC1 Vs PC2",PCDiabetes[,1], PCDiabetes[,2], pch=19,col=my_cols[class])
points(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=20)
plot(xlab="PC1 Vs PC2",PCDiabetes[,1], PCDiabetes[,2], pch=19,col=my_cols[class])
points(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=20)
plot(xlab="PC2 Vs PC3",PCDiabetes[,2], PCDiabetes[,3], pch=19,col=my_cols[class])
points(xlab="PC2 Vs PC3",PCDiabetes[chain3,2], PCDiabetes[chain3,3], pch=20)
plot(xlab="PC1 Vs PC2",PCDiabetes[,1], PCDiabetes[,2], pch=19,col=my_cols[class])
plot(xlab="PC1 Vs PC2",PCDiabetes[,1], PCDiabetes[,2], pch=19,col=my_cols[class])
points(xlab="PC1 Vs PC2",PCDiabetes[chain3,1], PCDiabetes[chain3,2], pch=20)
plot(xlab="PC1 Vs PC3",PCDiabetes[,1], PCDiabetes[,3], pch=19,col=my_cols[class])
points(xlab="PC1 Vs PC3",PCDiabetes[chain3,1], PCDiabetes[chain3,3], pch=20)
plot(xlab="PC2 Vs PC3",PCDiabetes[,2], PCDiabetes[,3], pch=19,col=my_cols[class])
points(xlab="PC2 Vs PC3",PCDiabetes[chain3,2], PCDiabetes[chain3,3], pch=20)
DEV.OFF()
dev.off()
############################ DECIDE VALUE FOR K-MEANS ##############################
# Let's see what happens if we use higher values for K
## WSS
fviz_nbclust(diabetes[-9],kmeans,method="wss",k.max=10)
############################ DECIDE VALUE FOR K-MEANS ##############################
# Let's see what happens if we use higher values for K
## WSS
fviz_nbclust(diabetes[-9],kmeans,method="wss",k.max=10)
############################ DECIDE VALUE FOR K-MEANS ##############################
# Let's see what happens if we use higher values for K
## WSS
fviz_nbclust(diabetes[-9],kmeans,method="wss",k.max=10)
## Silhouette
fviz_nbclust(diabetes[-9],kmeans,method="silhouette",k.max=10)
## Gap
fviz_nbclust(diabetes[-9],kmeans,method="gap",k.max=10,nboot=100)
############################ DECIDE VALUE FOR K-MEANS ##############################
# Let's see what happens if we use higher values for K
## WSS
fviz_nbclust(diabetes[-9],kmeans,method="wss",k.max=10)
## Silhouette
fviz_nbclust(diabetes[-9],kmeans,method="silhouette",k.max=10)
## Silhouette
fviz_nbclust(diabetes[-9],kmeans,method="silhouette",k.max=10)
## Gap
fviz_nbclust(diabetes[-9],kmeans,method="gap",k.max=10,nboot=100)
## Silhouette
fviz_nbclust(diabetes[-9],kmeans,method="silhouette",k.max=10)
## Gap
fviz_nbclust(diabetes[-9],kmeans,method="gap",k.max=10,nboot=100)
###################### MODEL-BASED CLUSTERING ########################################
## The idea behind this method is that each one of the classes or clusters follow a different
## probability distribution. In our case when looking at the plot PC1 vs PC2 it looks like
## the tested_positive samples might follow a uniform distribution while the tested_negative
## might follow a gamma distribution.
BIC.diabetes <- mclustBIC(diabetes[-9],G=1:5)
library(mclust)
BIC.NCI60 <- mclustBIC(X.NCI60,G=1:5)
BIC.NCI60
?mclustModelNames
plot(BIC.NCI60)
###################### MODEL-BASED CLUSTERING ########################################
## The idea behind this method is that each one of the classes or clusters follow a different
## probability distribution. In our case when looking at the plot PC1 vs PC2 it looks like
## the tested_positive samples might follow a uniform distribution while the tested_negative
## might follow a gamma distribution.
BIC.diabetes <- mclustBIC(diabetes[-9],G=1:5)
BIC.diabetes
plot(BIC.diabetes)
################################SET WORKING DIR###########################
#setwd("/Users/Jaime/Desktop/Master/PredictiveModeling/Project2/")
setwd("C:/Users/alvaro/Documents/GitHub/Predictive2/Project2Predictive")
################################IMPORTING LIBRARIES#######################
library("psych")
library("MASS")
library("scatterplot3d")
library("pracma")
library("tidyverse")
library("car")
library("glmnet")
##################################READ DATA##########################
admision = read.csv("Admission_Predict.csv",header = TRUE)
summary(admision)
# There are no missing values
# Get rid of serial number since it doesn't really mean anything
admision=admision[,c(-1)]
admision$Research=as.factor(admision$Research)
options(warn=-1)
names(admision)
names(admision)=c("GRE","TOEFL","UniRating","SOP","LOR","CGPA","Research","Chance")
attach(admision)
############################EXPLORATORY ANALYSIS#####################
pairs.panels(admision,
method = "pearson", # correlation method
hist.col = "#00146E",
col = "red",
lm = FALSE,
ellipses = FALSE,
smooth = FALSE,
#pch = c(21,21)[class],
#bg=my_cols[class],
rug = FALSE,
cex.cor = 5,
scale = TRUE,
density = TRUE  # show density plots
)
# The parameters included are :
## 1. GRE Scores (out of 340) GRE Quantitative variable
summary(GRE)
boxplot(GRE)
plot(ecdf(GRE))
plot(density(GRE))
## 2. TOEFL Scores (out of 120) TOEFL Quantitative variable
summary(TOEFL)
boxplot(TOEFL)
plot(ecdf(TOEFL))
plot(density(TOEFL))
## 3. University Rating (out of 5) UniRating Qualitative variable (it classifies Universities
## according to how good they are)
summary(UniRating)
boxplot(UniRating)
#admision$UniRating <- as.factor(UniRating)
# Does'nt make sense in qualitative variables as much as in continuous.
plot(ecdf(UniRating))
plot(density(UniRating))
## 4. Statement of Purpose (out of 5 every 0.5) SOP Quantitative variable that measures the goodnes of
## the statement of purpose
summary(SOP)
boxplot(SOP)
#admision$SOP <- as.factor(SOP)
# Does'nt make sense in qualitative variables as much as in continuous.
plot(ecdf(SOP))
plot(density(SOP))
## 5. Letter of Recommendation Strength (out of 5 every 0.5) LOR Quantitative variable (Very similar to
## the previous one but concerning letters of recommendation)
summary(LOR)
boxplot(LOR)
#admision$LOR <- as.factor(LOR)
# Does'nt make sense in qualitative variables as much as in continuous.
plot(ecdf(LOR))
plot(density(LOR))
## 6. Undergraduate GPA (out of 10) CGPA Quantitative variable
summary(CGPA)
# Does'nt make sense in qualitative variables as much as in continuous.
plot(ecdf(CGPA))
boxplot(CGPA)
plot(density(CGPA))
## 7. Research Experience (either 0 or 1) Research Binary variable
summary(Research)
## 8. Chance of Admit (ranging from 0 to 1) Chance Quantitative variable
summary(Chance)
boxplot(Chance)
plot(ecdf(Chance))
plot(density(Chance))
## This variable, as is CGPA, is a little bit skewed, but it could be assumed to follow
## a Normal distribution. Also, since this is going to be our target variable, we will
## make it a binary one by using a cutoff.
attach(admision)
################################LINEAR REGRESSION#########################
Anova(lm(Chance~.,data = admision))
## In reality Chance is a continuous variable so it is interesting to see from what
## variables it could be predicted.
## Computing the anova table suggests that the most important variables are GRE, TOEFL, LOR, CGPA
## and Research.
## Let's see what the best linear model is according to the BIC just to see what are the most
## important variables.
model <- stepAIC(lm(Chance~.,data=admision), k = log(length(Chance)))
summary(model)
#############################LOGISTIC REGRESSION###########################
## For the reasons given before we will be building some classification models, where
## the class will be given by Chance>0.9, i.e. having more than 90% probability of
## being accepted.
plot(density(Chance))
abline(v=0.9)
cutoff=0.9
summary(Chance > cutoff)
#### Let's start by looking at Chance vs. each one of the variables
diff_model<-function(response,predictor,name="",data=admision,gr=FALSE){
predictor_scaled=scale(predictor)
mod <- glm(response>cutoff~predictor_scaled,family = "binomial",data=admision)
if (gr == TRUE) {
x_plot <- seq(-3,3,by=0.1)
temp <- mod$coefficients[1]+mod$coefficients[2]*x_plot
x_plot <- x_plot*sd(predictor)+mean(predictor)
plot(x=x_plot,y=logistic(temp),type = "line",xlab=name,ylab = "Probability")
points(x=predictor,y=response>cutoff)
x_d <- (-mod$coefficients[1]/mod$coefficients[2])*sd(predictor)+mean(predictor)
y_d <- 0.5
points(x_d,y_d,pch=19,col="blue")
text(x_d,y_d,labels = round(x_d,2),pos=4)
}
tab <- table(Chance>cutoff,mod$fitted.values>0.5)
print(tab)
accuracy <- sum(diag(tab)) / sum(tab)
tnr <- tab[1]/sum(tab[,1])
tpr <- tab[4]/sum(tab[,2])
print(name)
print(paste("Accuracy:",accuracy))
print(paste("True positive rate:",tpr))
print(paste("True negative rate:",tnr))
}
diff_model(Chance,GRE,"GRE", gr=TRUE)
diff_model(Chance,TOEFL,"TOEFL", gr=TRUE)
##diff_model(Chance,Research+GRE,"aa")
indexes <- c(1,2,4,5,6)
for (i in 1:(length(indexes))) {
for (j in 1:(length(indexes))) {
if (j > i) {
diff_model(Chance,admision[indexes[i]]+admision[indexes[j]], paste(names(admision)[indexes[i]],"+",names(admision)[indexes[j]]))
}
}
}
## BEST MODEL ACCORDING TO BIC
mod <- stepAIC(glm(Chance > cutoff ~ ., data = admision, family = "binomial"), k = log(length(Chance)))
summary(mod)
pairs.panels(admision[,c(3,6,7,8)],
method = "pearson", # correlation method
hist.col = "#00146E",
col = "red",
lm = FALSE,
ellipses = FALSE,
smooth = FALSE,
rug = FALSE,
cex.cor = 5,
scale = TRUE,
density = TRUE  # show density plots
)
tab <- table(Chance>cutoff,mod$fitted.values>0.5)
print(tab)
accuracy <- sum(diag(tab)) / sum(tab)
tnr <- tab[1]/sum(tab[,1])
tpr <- tab[4]/sum(tab[,2])
print(paste("Accuracy:",accuracy))
print(paste("True positive rate:",tpr))
print(paste("True negative rate:",tnr))
## Allowing for interactions returns the same result -> The important predictors are GRE, LOR and CGPA
mod_int <- stepAIC(glm(Chance > cutoff ~ .^2, data=admision, family="binomial"), k = log(length(Chance)))
summary(mod_int)
summary(mod)
confint.default(mod, level=0.95)
exp(confint.default(mod, level=0.95))
exp(confint.default(mod, level=0.95))
diff_model<-function(response,predictor,name="",data=admision){
predictor_scaled=scale(predictor)
mod=glm(response>cutoff~predictor_scaled,family = "binomial",data=admision)
x_plot=seq(-3,3,by=0.1)
temp=mod$coefficients[1]+mod$coefficients[2]*x_plot
x_plot=x_plot*sd(predictor)+mean(predictor)
plot(x=x_plot,y=logistic(temp),type = "line",xlab=name,ylab = "Probability")
points(x=predictor,y=response>cutoff)
x_d=(-mod$coefficients[1]/mod$coefficients[2])*sd(predictor)+mean(predictor)
y_d=0.5
points(x_d,y_d,pch=19,col="blue")
text(x_d,y_d,labels = round(x_d,2),pos=4)
tab=table(Chance>cutoff,mod$fitted.values>0.5)
print(tab)
accuracy <- sum(diag(tab)) / sum(tab)
print(paste("Accuracy:",accuracy))
}
mod
#################################ANALYSING DEVIANCE##############################
mod
#################################ANALYSING DEVIANCE##############################
mod$deviance
names(mod)
mod$R
help(R)
R_squared=1-(mod$deviance/mod$null.deviance)
R_squared
names(mod)
mod$anova
#################################ANALYSING DEVIANCE##############################
mod$deviance
mod$null.deviance
R_squared=1-(mod$deviance/mod$null.deviance)
#it is a proportion of how good the fit is compared to the worst
R_squared
#it is a proportion of how good the fit is compared to the worst
R_squared
names(mod)
mod$anova
mod$anova
############################MODEL DIAGNOSIS########################
plot(mod,2)
############################MODEL DIAGNOSIS########################
plot(mod,2)
############################MODEL DIAGNOSIS########################
plot(mod,1)
plot(mod,2)
plot(mod,3)
plot(mod,4)
plot(mod,5)
plot(mod,6)
plot(mod,7)
plot(mod,2)
